{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7482b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "#from sqlalchemy import create_engine\n",
    "import pyhive\n",
    "from sqlalchemy.engine import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f525ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presto\n",
    "engine = create_engine('presto://localhost:8080/system/runtime') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9751b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Presto Data query into a DataFrame\n",
    "# df = pd.read_sql('select count(*) from sword_purchases', engine)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f374c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyhive import presto\n",
    "# cursor = presto.connect(host='localhost', port=8080).cursor()\n",
    "# statement = 'select count(*) from sword_purchases'\n",
    "# cursor.execute(statement)\n",
    "# my_results = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d978cdb4",
   "metadata": {},
   "source": [
    "### Summary Instructions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a85e1",
   "metadata": {},
   "source": [
    "**(1). Activate Docker Compose Cluster:**\n",
    "```\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "**(2). Run Game Application:**\n",
    "```\n",
    "docker-compose exec mids env FLASK_APP=/w205/project-3-cmorenoUCB2021/game_api.py flask run --host 0.0.0.0\n",
    "```\n",
    "\n",
    "**(3). Set up to Watch Kafka:** to observe how messages are being captured. Open a new terminal, and run the following (run twice):\n",
    "```\n",
    "docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning\n",
    "```\n",
    "\n",
    "**(4). Run Application to Read Messages from Kafka and write them to hdfs:** using separate terminals, run applications to read Messages from Kafka associated with the key events from the game as follows:\n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;**(a) do_nothing:**\n",
    "```\n",
    "```\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**(b) purchase_a_sword:**\n",
    "```\n",
    "docker-compose exec spark spark-submit /w205/project-3-cmorenoUCB2021/write_swords_stream.py\n",
    "```\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**(c) purchase_a_knife:**\n",
    "```\n",
    "```\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**(d) purchase_a_shield:**\n",
    "```\n",
    "```\n",
    "\n",
    "**(4). Check what was written in Hadoop:** the following command would check what was written for sword_purchases. \n",
    "Note: check files for each event (for sanity check).\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/sword_purchases\n",
    "```\n",
    "\n",
    "**(5). Set up Presto:**\n",
    "\n",
    "*(a). Run hive in hadoop container:*\n",
    "```\n",
    "docker-compose exec cloudera hive\n",
    "```\n",
    "*(b). Create table:*\n",
    "```\n",
    "create external table if not exists default.sword_purchases (\n",
    "    raw_event string,\n",
    "    timestamp string,\n",
    "    Accept string,\n",
    "    Host string,\n",
    "    User_Agent string,\n",
    "    event_type string\n",
    "    \n",
    "  )\n",
    "  stored as parquet \n",
    "  location '/tmp/sword_purchases'\n",
    "  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "```\n",
    "```\n",
    "create external table if not exists default.sword_purchases (Accept string, Host string, User_Agent string, event_type string, timestamp string, raw_event string) stored as parquet location '/tmp/sword_purchases'  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "```\n",
    "\n",
    "**Note:** `ctrl-D` to exit the hive shell.\n",
    "\n",
    "**(6). Query Tables with Presto:**  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**(a). Run Presto:**\n",
    "```\n",
    "docker-compose exec presto presto --server presto:8080 --catalog hive --schema default\n",
    "```\n",
    "&nbsp;&nbsp;&nbsp;**(b). Examples of Queries with Presto:**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;-What tables are there in Presto?\n",
    "```\n",
    "presto:default> show tables;\n",
    "```\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;-Describe one of the tables (sword_purchases):\n",
    "```\n",
    "presto:default> describe sword_purchases;\n",
    "```\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;-Query `purchases` table:  \n",
    "```\n",
    "presto:default> select * from sword_purchases;\n",
    "```\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;-Count the number of events in `purchases` table:  \n",
    "```\n",
    "presto:default> select count(*) from sword_purchases;\n",
    "```\n",
    "\n",
    "**(7). Use Apache Bench to generate data:**  \n",
    "```\n",
    "docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/\n",
    "docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_a_sword\n",
    "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/\n",
    "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_a_sword\n",
    "```\n",
    "  \n",
    "*Another way to feed data to the stream:*  \n",
    "```\n",
    "while true; do\n",
    "  docker-compose exec mids \\\n",
    "    ab -n 10 -H \"Host: user1.comcast.com\" \\\n",
    "      http://localhost:5000/purchase_a_sword\n",
    "  sleep 10\n",
    "done\n",
    "```\n",
    "```\n",
    "while true; do docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_a_sword; sleep 10; done\n",
    "```\n",
    "\n",
    "**(8). HOW TO RUN PRESTO IN JUPYTER NOTEBOOK:?**\n",
    "- ** Run spark to make sure it is available when running Jupyter Notebook:**\n",
    "\n",
    "```\n",
    "docker-compose exec spark ln -s /w205 w205\n",
    "```\n",
    "\n",
    "- **Run Jupyter Notebook in Google Cloud to read kafka topic and explore data:**\n",
    "```\n",
    "docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 7000 --ip 0.0.0.0 --allow-root' pyspark\n",
    "```\n",
    "\n",
    "- **Get the token and include the address for your notebook instance in Google Cloud:** For example:\n",
    "```\n",
    "http://34.139.108.62:7000/?token=9d42832da7a128cbc081a3eb4a5f30b70c853987d8666395\n",
    "```\n",
    "Replace 0.0.0.0 with the address associated to your Google Cloud Instance.\n",
    "\n",
    "34.139.108.62"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25214bfd",
   "metadata": {},
   "source": [
    "## APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd752ce",
   "metadata": {},
   "source": [
    "### I. Docker Compose Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5908796",
   "metadata": {},
   "source": [
    "```\n",
    "version: '2'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 32181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    expose:\n",
    "      - \"2181\"\n",
    "      - \"2888\"\n",
    "      - \"32181\"\n",
    "      - \"3888\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    expose:\n",
    "      - \"9092\"\n",
    "      - \"29092\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  cloudera:\n",
    "    image: midsw205/hadoop:0.0.2\n",
    "    hostname: cloudera\n",
    "    expose:\n",
    "      - \"8020\" # nn\n",
    "      - \"8888\" # hue\n",
    "      - \"9083\" # hive thrift\n",
    "      - \"10000\" # hive jdbc\n",
    "      - \"50070\" # nn http\n",
    "    ports:\n",
    "      - \"8888:8888\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  spark:\n",
    "    image: midsw205/spark-python:0.0.6\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    expose:\n",
    "      - \"8888\"\n",
    "    ports:\n",
    "      - \"8889:8888\" # 8888 conflicts with hue\n",
    "    depends_on:\n",
    "      - cloudera\n",
    "    environment:\n",
    "      HADOOP_NAMENODE: cloudera\n",
    "      HIVE_THRIFTSERVER: cloudera:9083\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "    command: bash\n",
    "\n",
    "  presto:\n",
    "    image: midsw205/presto:0.0.1\n",
    "    hostname: presto\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    expose:\n",
    "      - \"8080\"\n",
    "    environment:\n",
    "      HIVE_THRIFTSERVER: cloudera:9083\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  mids:\n",
    "    image: midsw205/base:0.1.9\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    expose:\n",
    "      - \"5000\"\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599990e",
   "metadata": {},
   "source": [
    "### II. Game Application - Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dcffd4",
   "metadata": {},
   "source": [
    "```\n",
    "#!/usr/bin/env python\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "producer = KafkaProducer(bootstrap_servers='kafka:29092')\n",
    "\n",
    "\n",
    "def log_to_kafka(topic, event):\n",
    "    event.update(request.headers)\n",
    "    producer.send(topic, json.dumps(event).encode())\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def default_response():\n",
    "    default_event = {'event_type': 'default',\n",
    "                     'name': 'doing_nothing',\n",
    "                     'strength':'NA',\n",
    "                     'price': 'NA'}\n",
    "    log_to_kafka('events', default_event)\n",
    "    return \"What are you waiting for?\\n\"\n",
    "\n",
    "\n",
    "@app.route(\"/purchase_a_sword\", methods=['POST','GET'])\n",
    "    \"\"\"\n",
    "    @function: This function generate a Purchase a Sword event from a user mobile device request or Apache Bench\n",
    "    @param: User Request (via URL endpoint) \n",
    "    @return: Returns string of User Id and Event \n",
    "    \"\"\"\n",
    "def purchase_a_sword():\n",
    "    userid = request.args.get('userid', default='001', type=str)\n",
    "    n = request.args.get('n',default=1,type=int)\n",
    "    purchase_sword_event = {'userid':userid,\n",
    "                            'event_type': 'purchase_sword',\n",
    "                            'name': 'excalibur',\n",
    "                            'strength': '1000',\n",
    "                            'n_purchases': n,\n",
    "                            'price': 2000}\n",
    "    log_to_kafka('events', purchase_sword_event)\n",
    "    return \"USER \" + userid + \": \"+ str(n)+\" \"+ \" Sword(s) Purchased!\\n\"\n",
    "\n",
    "\n",
    "@app.route(\"/join_guild/\", methods=['POST','GET'])\n",
    "def join_guild():\n",
    "    \"\"\"\n",
    "    @function: This function generate a Join Guild event from a user mobile device request or Apache Bench\n",
    "    @param: User Request (via URL endpoint) \n",
    "    @return: Returns string of User Id and Event \n",
    "    \"\"\"\n",
    "    userid = request.args.get('userid', default='001', type=str)\n",
    "    guild_name = request.args.get('guild_name',default=\"Knights of the Round Table\",type=str)\n",
    "    join_guild_event = {'userid': userid,\n",
    "                        'event_type': 'join_guild',\n",
    "                        'name': guild_name,\n",
    "                        'strength': 1200,\n",
    "                        'n_purchases': '1',\n",
    "                        'price': 1000}\n",
    "    log_to_kafka('events', join_guild_event)\n",
    "    return \"Joined\" +\" \"+ guild_name +\" \"+ \"Guild!\\n\"\n",
    "\n",
    "\n",
    "@app.route(\"/purchase_a_knife\")\n",
    "def purchase_a_knife():\n",
    "    userid = request.args.get('userid', default='001', type=str)\n",
    "    n = request.args.get('n',default=1,type=int)\n",
    "    purchase_knife_event = {'userid': userid,\n",
    "                            'event_type': 'purchase_knife',\n",
    "                            'name': 'kukri',\n",
    "                            'strength': 500,\n",
    "                            'n_purchases': n,\n",
    "                            'price': 1000}\n",
    "    log_to_kafka('events', purchase_knife_event)\n",
    "    return \"USER \" + userid + \": \"+ str(n)+\" \"+ \" Kniefe(s) Purchased!\\n\"\n",
    "\n",
    "@app.route(\"/purchase_a_shield\")\n",
    "def purchase_a_shield():\n",
    "    userid = request.args.get('userid', default='001', type=str)\n",
    "    n = request.args.get('n',default=1,type=int)\n",
    "    purchase_shield_event = {'userid': useride,\n",
    "                             'event_type': 'purchase_shield',\n",
    "                             'name': 'parma',\n",
    "                             'strength': 800,\n",
    "                             'n_purchases': n,\n",
    "                             'price': 1500}\n",
    "    log_to_kafka('events', purchase_shield_event)\n",
    "    return \"USER \" + userid + \": \"+ str(n)+\" \"+ \" Shield(s) Purchased!\\n\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb7f62",
   "metadata": {},
   "source": [
    "### III. Applications to Extract events from kafka and write them to hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7a88b",
   "metadata": {},
   "source": [
    "#### Application to Read and Write Event Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9104ee2",
   "metadata": {},
   "source": [
    "```\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Extract events from kafka and write them to hdfs\n",
    "\"\"\"\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "def purchase_sword_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_sword_purchase(event_as_json):\n",
    "    \"\"\"udf for filtering events\n",
    "    \"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"main\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ExtractEventsJob\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    raw_events = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"events\") \\\n",
    "        .load()\n",
    "\n",
    "    sword_purchases = raw_events \\\n",
    "        .filter(is_sword_purchase(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          purchase_sword_event_schema()).alias('json')) \\\n",
    "        .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "    sink = sword_purchases \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_sword_purchases\") \\\n",
    "        .option(\"path\", \"/tmp/sword_purchases\") \\\n",
    "        .trigger(processingTime=\"120 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    sink.awaitTermination()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e80302",
   "metadata": {},
   "source": [
    "#### Useful References\n",
    "- https://towardsdatascience.com/jupyter-magics-with-sql-921370099589"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb58968c",
   "metadata": {},
   "source": [
    "**Imports for working with Presto:**\n",
    "pip install pandas\n",
    "pip install sqlalchemy # ORM for databases\n",
    "pip install ipython-sql # SQL magic function"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m78"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
